{
  "interviewId": "0a5818e2-7ace-496f-a0f7-d67cfa735eef",
  "caseStudyId": "case_tech_engagement_drop_2025",
  "caseStudyTitle": "TechConnect: Declining User Engagement",
  "candidateName": "Yvo",
  "timestamp": "2025-04-09T16:24:12.622Z",
  "conversation": [
    {
      "role": "INTERVIEWER",
      "content": "Hello Yvo, it's great to have you here today. I'm excited to help you move closer to your dream job while we go through this case interview. I'll briefly explain the process: I'll begin by presenting you with a business case, and then we'll discuss your thought process and analysis as you work through it. \n\nHere's your case: Your client is TechConnect, a popular social media platform with 50 million monthly active users (MAU). Over the past six months, key engagement metrics—daily active users (DAU) as a percentage of MAU, average time spent per session, and content interaction rates (likes, comments, shares per user)—have all declined by 15-20%. In tandem, ad-based revenue has also started to decline. The Head of Product has engaged your firm to diagnose the root cause of the engagement decline and recommend corrective actions.\n\nCould you please share your initial thoughts on how you would approach this case?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Hello, thank you for the opportunity to work on this case today. I appreciate the chance to dive into TechConnect's issue.\n\nBefore I jump into structuring my approach, I’d like to clarify one key point: Has there been any notable product change (for example, feature updates, algorithm adjustments, UI modifications) or external event coinciding with the start of this six-month decline period?\n\nAssuming for now that we might uncover such changes, my initial approach would involve these areas:\n\n1. Probing Product & User Experience:\n • Assess recent changes to the platform (e.g., new features, algorithms) that might affect user engagement.\n • Analyze data segmented by user demographics and behavior to pinpoint if certain groups are more affected.\n\n2. Evaluating External & Competitive Factors:\n • Benchmark TechConnect’s engagement metrics against competitors over the same period.\n • Examine any shifts in market trends or emerging platforms that might be attracting users.\n\n3. Investigating Monetization Dynamics:\n • Look at the interplay between declining engagement and ad revenue, such as changes in ad targeting or reduced advertiser spending.\n • Analyze if lowered ad performance is a cause or effect of decreased on-platform engagement.\n\nWith these pillars in mind, I’d develop and test hypotheses — for instance, my initial hypothesis is that a recent product change may have inadvertently reduced user engagement, which in turn is impacting ad revenue. Could we explore more details about any product modifications during this period?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "INTERVIEWER",
      "content": "Great, Yvo. Let's dive deeper into the product and user experience pillar you mentioned. If there indeed was a product change during this period, could you explain which specific user engagement metrics you'd analyze and how you’d structure the analysis to isolate the impact of that change?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Certainly. Given a product change during this period, I'd start by honing in on a few key engagement metrics and structuring my analysis around a few intertwined steps.\n\n1. Key Engagement Metrics:\n • DAU/MAU Ratio: Since it’s a direct measure of daily engagement relative to the overall user base, I'd track changes in this ratio across different user cohorts before and after the change.\n • Average Time Spent per Session: This metric helps indicate user interest—if users are spending less time, it might suggest that the new product feature isn't resonating.\n • Content Interaction Rates: Likes, comments, and shares per user are vital to gauge genuine engagement; a decline may point to a disruption in the content discovery or interaction flow.\n\n2. Analysis Structure:\n a. Cohort Analysis:\n  • Segment users based on their exposure to the product change (e.g., users who experienced the change vs. those who haven’t if any “control” group exists, perhaps through phased roll-outs).\n  • Analyze trends over time—for each cohort—to see if the decline correlates closely with the product change.\n\n b. Time-Series and Pre/Post Comparison:\n  • Conduct a pre-post analysis comparing the periods before and after the implementation of the product change.\n  • Use difference-in-differences if there's an appropriate control group, which can help isolate the effect of the product change from other temporal factors.\n\n c. Diving Deeper with Segmentation:\n  • Break down metrics by user demographics (e.g., age, location) and behavior (e.g., new versus existing users). This could reveal if specific segments are more adversely affected.\n  • For example, it might be that new users, unfamiliar with previous interfaces, are more impacted compared to loyal users.\n\n3. Hypothesis Testing:\n • My working hypothesis is that the product change has inadvertently introduced friction into the user experience, reducing engagement.\n • I’d test this by checking if the decline in metrics like average session time and content interactions statistically aligns with the period following the product change.\n • If possible, I’d also look into qualitative data, such as user feedback or survey responses, to get direct insights into perceived challenges.\n\nBy isolating these dimensions, we should be able to confirm or refute if the product change is the primary driver behind the observed decline. Does this approach and the specific metrics I’d focus on align with your expectations, or would you like to explore additional angles?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "INTERVIEWER",
      "content": "Good insights, Yvo. As a follow-up, if you discover that the decline is significantly more pronounced among new users, how would you further investigate their onboarding experience to validate your hypothesis? Could you also suggest any additional qualitative or experimental methods you might use to gather deeper insights into these users' experiences?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Thank you for the follow-up question. Focusing on new users, here's how I’d proceed to investigate the onboarding experience:\n\n1. Map the Onboarding Funnel:\n • I’d start by breaking down the onboarding process into key steps—account creation, initial profile setup, tutorial walkthrough, discovery of core features—and analyzing conversion rates at each stage.\n • I’d look at metrics such as the drop-off rate between these steps, the time spent on each phase, and the percentage of users who complete the onboarding process. This will help us pinpoint exactly where new users are stumbling.\n\n2. Qualitative Methods:\n • Conduct User Interviews: I’d arrange targeted interviews or focus groups with new users to gather firsthand feedback. Questions would probe frustrations, perceived complexity, and clarity of instructions.\n • Usability Testing: Setting up moderated sessions where new users complete the onboarding steps can help us observe real-time issues, friction points, or confusing elements.\n • Survey Data: A brief exit survey at dropout points could capture immediate feedback regarding why they’re not proceeding.\n\n3. Experimental Methods:\n • A/B Testing: I’d recommend testing variations of the onboarding process. For instance, one group could receive an enhanced tutorial with clearer calls to action or additional support, while the other experiences the standard process. Comparing their completion and engagement rates would provide causal insights.\n • Heat Mapping: If available, using heat mapping tools during the onboarding phase can identify where users linger or get confused on the interface.\n\n4. Additional Data Points:\n • Feature Adoption: Track which core features new users engage with post-onboarding. A very low adoption rate might indicate that the onboarding fails to convey the value of the product effectively.\n • User Segmentation: Differentiate between various new user cohorts (e.g., by demographics, referral source) to see if specific groups are more negatively impacted.\n\nBy combining these quantitative and qualitative approaches, we would validate if and why the new user onboarding experience is contributing to the decline in engagement. This data-driven insight would then inform targeted interventions, such as revising the onboarding flow, simplifying tutorials, or personalizing the onboarding experience.\n\nMy recommendation moving forward would be to pilot an optimized onboarding process with robust A/B testing and gather continuous user feedback. This iterative process should help in both confirming our hypothesis and uncovering actionable improvements in the onboarding experience.",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "INTERVIEWER",
      "content": "Great, Yvo. Your approach to investigating new user onboarding is very structured. As a follow-up, once you implement changes, how would you measure the success of the revised onboarding process? In particular, what key performance indicators (KPIs) and thresholds would you establish to ensure that these improvements positively impact overall user engagement and retention?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Thank you for the question. After implementing changes to the onboarding process, I’d focus on both direct onboarding metrics and downstream engagement indicators to capture the full impact of the improvements. Here’s how I’d structure the measurement:\n\n1. Direct Onboarding KPIs:\n • Conversion Rate at Each Onboarding Stage: This includes the percentage of new users completing account creation, the tutorial, and reaching the “aha moment” (i.e., first key engagement action). For example, if the baseline tutorial completion rate was 50%, I’d target at least a 10–15% improvement.\n • Time-to-Activation: Track the time it takes new users to complete onboarding and start engaging with core features. A decrease in time-to-activation often correlates with better user comprehension and quicker value realization.\n\n2. Engagement & Retention Metrics:\n • Early Retention Rates: Monitor 7-day and 30-day retention rates for users exposed to the revised onboarding. An improvement in 7-day retention by, say, 5–10% could indicate that users are finding value early on.\n • Initial Engagement Levels: Metrics such as DAU/MAU ratio or the number of sessions per user in the first month can signal whether users are integrating into the platform more effectively.\n • Content Interaction Rates: As onboarding improves, users should interact more with core platform content. An upward trend in likes, comments, or shares from newly onboarded users can be a positive signal.\n\n3. Experimental Confirmation:\n • A/B Testing Outcomes: Compare the group undergoing the new onboarding flow versus a controlled group. I’d define thresholds for success such as:\n  – A statistically significant increase in conversion at key steps (e.g., from tutorial finish to first meaningful interaction).\n  – Improved retention metrics over a set period (e.g., a 7-day retention improvement that is statistically significant).\n  – Better engagement outputs, ensuring that any observed uplifts are maintained over time.\n • Qualitative Feedback: Continuous monitoring of user satisfaction through surveys or in-app feedback can also help in understanding if changes resonate with users.\n\nThese KPIs, combined with established improvement thresholds—like a 10–15% increase in key conversion rates and a similar magnitude uplift in early retention—will help ensure that the revised onboarding process is making a positive impact. If these indicators show the expected improvement, it suggests that the new onboarding flow is effective; if not, it would signal the need for further iterative adjustments.\n\nIn summary, by monitoring both immediate onboarding metrics and subsequent user engagement and retention, we can confidently determine if our improvements are driving the desired business outcomes.",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "INTERVIEWER",
      "content": "That's a very solid approach, Yvo. By using conversion rates, time-to-activation, retention, and engagement metrics, along with ongoing A/B testing and qualitative feedback, you'll be well-equipped to measure whether the revised onboarding process is indeed driving improvements. Can you discuss how you might adjust your strategy if you observe improvements in onboarding metrics but little change in long-term user retention and engagement?",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Thank you for that question. If we see that the onboarding metrics have improved (for example, higher completion rates and faster time-to-activation) but long-term retention and engagement remain stagnant, it suggests that while we're successfully getting users onboarded, something in the subsequent user journey isn't maintaining their interest. Here's how I'd adjust the strategy:\n\n1. Analyze Post-Onboarding Funnel:\n • Map the entire user journey beyond onboarding to identify where drop-offs occur. Are there specific friction points, such as confusing features, lack of engaging content, or poor personalization right after onboarding?\n • Conduct a cohort analysis that tracks new users over time to see if engagement diminishes after a specific period or user action.\n\n2. Investigate Product Experience and Content Value:\n • Evaluate the relevance and quality of the core content or features that drive long-term engagement. It's possible that even though users are onboarded successfully, the ongoing content or functionality isn’t compelling enough.\n • Look at personalization efforts—are new users receiving content that’s tailored to their interests? Do they have opportunities to engage with communities or network with others?\n\n3. Gather Further Qualitative Feedback:\n • Conduct interviews and surveys with users who have completed onboarding but then disengage. This might reveal if their expectations set during onboarding aren’t met later on or if there are usability issues further along.\n • Use in-app feedback mechanisms to capture real-time insights on what might be lacking after the initial excitement.\n\n4. Experiment with Post-Onboarding Engagement Initiatives:\n • Consider pilot programs such as enhanced product tutorials or guided tours that extend beyond the onboarding phase.\n • Test personalized follow-ups like notifications, content recommendations, or community-building initiatives aimed at re-engaging users.\n • Implement A/B testing to compare varied engagement strategies and measure their impact on longer-term retention.\n\n5. Update the Overall User Experience Framework:\n • If the problem persists, we might need a holistic review of the user experience that goes beyond onboarding—ensuring that the platform continually delivers value. This may involve refining the user interface, refreshing content algorithms, or even introducing features aimed at recurring engagement.\n\nIn summary, if onboarding improvements aren’t translating into sustained engagement, the key next step is to broaden our analysis to the entire customer journey. By identifying where users lose value after the initial sign-up and systematically testing enhancements in those areas, we can drive improvements in long-term retention and engagement. My recommendation would be to launch a series of cross-functional initiatives focusing on post-onboarding experience enhancements, closely monitor their impact, and adjust continuously based on both quantitative metrics and qualitative insights.",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "INTERVIEWER",
      "content": "Great response, Yvo. In that situation, I'd first conduct an in-depth analysis of the post-onboarding journey to identify specific drop-off points; for instance, examining if users lose interest after an initial feature or if there's a gap in the user experience that prevents sustained value delivery. \n\nNext, I’d recommend gathering qualitative feedback—through direct interviews or surveys—with those users to understand their unmet expectations, and testing targeted interventions (like personalized content recommendations or post-onboarding engagement initiatives) to sustain interest.",
      "timestamp": "2025-04-09T16:24:12.622Z"
    },
    {
      "role": "CANDIDATE",
      "content": "Thank you for that framing—it's an important scenario to consider. Here's how I'd adjust the strategy:\n\n1. Map and Analyze the Full User Journey:\n • First, I’d extend the analysis beyond onboarding to map out the entire post-onboarding funnel. This involves identifying key moments where user engagement dips—such as after the initial welcome or when transitioning into regular content consumption.\n • I'd conduct a cohort analysis to track the behavior of users from the moment they complete onboarding through various stages, pinpointing where retention starts to falter.\n\n2. Gather Additional Qualitative Insights:\n • Even with improved onboarding metrics, user feedback collected via exit surveys, interviews, or in-app feedback can give us clues as to why long-term engagement isn’t improving. For instance, it might reveal that while the initial setup is smooth, users feel the subsequent content isn’t personalized or lacks relevance.\n • I’d also run usability tests for users in the post-onboarding phase to directly observe friction points that might be unaddressed.\n\n3. Implement and Experiment with Post-Onboarding Interventions:\n • Based on the findings, I would pilot targeted initiatives—such as personalized content recommendations, enhanced user journey tutorials, or periodic check-ins to re-engage users.\n • A/B testing these interventions would be crucial: comparing a control group with users receiving these post-onboarding optimizations to quantify their impact on metrics like 30-day retention and DAU/MAU ratio.\n\n4. Evaluate and Iterate:\n • If the tests show improvements, we can scale the successful interventions—ensuring iterative development driven by both quantitative data (engagement/retention KPIs) and qualitative insights.\n • Simultaneously, we’d monitor whether these changes create a new friction point or if further iterations are needed. The ultimate goal is to sustain the excitement and value communicated during onboarding throughout the user’s lifecycle.\n\nIn summary, while improved onboarding tells us we’re capturing users effectively at the start, we must ensure they’re engaged over the long term by continuously delivering personalized, value-driven experiences. I would recommend launching a series of post-onboarding enhancements, with clear pilot tests and measurable KPIs such as improvement in 30-day retention rates and daily engagement metrics.",
      "timestamp": "2025-04-09T16:24:12.622Z"
    }
  ]
}